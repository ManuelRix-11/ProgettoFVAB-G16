{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T14:26:20.335225Z",
     "start_time": "2025-04-27T14:26:12.923848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from scipy.stats import ks_2samp\n",
    "from io import StringIO\n",
    "import json"
   ],
   "id": "7a9a2fc61c8b90c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Desktop\\Cosecose\\PR1\\TestBiometria\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config",
   "id": "1060c2779d546ac6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T14:26:20.833194Z",
     "start_time": "2025-04-27T14:26:20.585313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"qwen2.5:3b\"\n",
    "\n",
    "# Dataset di domande e risposte\n",
    "with open('qa_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "# Dati reali per confronto tabellare\n",
    "real_data = pd.read_excel(\"2022.11ÎË▒Ý▓╣│õÍð╬─Ê¹╩│/Shanghai_T1DM_Summary.xlsx\")"
   ],
   "id": "5df5b1433aeac589",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Funzioni per evaluation del qa",
   "id": "25f373ac5c9a4ec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T14:26:20.879785Z",
     "start_time": "2025-04-27T14:26:20.866260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Manda un prompt al modello Ollama e restituisce la risposta.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "def evaluate_qa(predictions, references):\n",
    "    \"\"\"Valuta domande e risposte con BERTScore.\"\"\"\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"it\")\n",
    "    return results\n",
    "\n",
    "def run_qa_evaluation():\n",
    "    print(\"\\n=== Valutazione Risposte a Domande ===\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for item in qa_dataset:\n",
    "        prompt = f\"Rispondi in modo conciso alla domanda: {item['question']}\"\n",
    "        response = query_ollama(prompt)\n",
    "        predictions.append(response.strip())\n",
    "        references.append(item['answer'])\n",
    "\n",
    "    results = evaluate_qa(predictions, references)\n",
    "    f1_scores = results['f1']\n",
    "    f1_medio = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    print(\"\\nBERTScore medio (F1):\", round(f1_medio, 4))"
   ],
   "id": "cf6d68d9fa0251ac",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_qa_evaluation()",
   "id": "5ce1726dc32713",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
